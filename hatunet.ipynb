{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import math # For PositionalEncoding2D if it's in the same file, otherwise import from model file\n",
    "\n",
    "# --- Assuming HybridAttentionTransformerUNet is in a file named model.py ---\n",
    "# Or paste the model definition (HybridAttentionTransformerUNet and its sub-modules) directly here\n",
    "# For this example, let's assume it's defined above or in an imported file.\n",
    "\n",
    "# --- Model Definition (Paste HybridAttentionTransformerUNet and its components here if not importing) ---\n",
    "# --- 1. Convolutional Block ---\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard Double Convolutional Block: (Convolution -> BatchNorm -> ReLU) * 2\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "# --- 2. Attention Gate ---\n",
    "class AttentionGate(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention Gate to focus on relevant features from skip connections.\n",
    "    \"\"\"\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super(AttentionGate, self).__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        return x * psi\n",
    "\n",
    "# --- 3. Transformer Encoder Block ---\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Transformer Encoder layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1):\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout_rate, batch_first=True)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(ff_dim, embed_dim),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        x = x + self.dropout(attn_output)\n",
    "        x = self.norm1(x)\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = x + self.dropout(ffn_output)\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "# --- 4. 2D Positional Encoding ---\n",
    "class PositionalEncoding2D(nn.Module):\n",
    "    \"\"\"\n",
    "    Adds 2D positional encodings to the input feature map.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, height, width):\n",
    "        super(PositionalEncoding2D, self).__init__()\n",
    "        if d_model % 4 != 0:\n",
    "            raise ValueError(\"Cannot use sin/cos positional encoding with odd dimension (got dim={:d})\".format(d_model))\n",
    "        pe = torch.zeros(d_model, height, width)\n",
    "        d_model_half = d_model // 2\n",
    "        div_term = torch.exp(torch.arange(0., d_model_half, 2) * -(math.log(10000.0) / d_model_half))\n",
    "        pos_w = torch.arange(0., width).unsqueeze(1)\n",
    "        pos_h = torch.arange(0., height).unsqueeze(1)\n",
    "\n",
    "        pe[0:d_model_half:2, :, :] = torch.sin(pos_w * div_term).transpose(0, 1).unsqueeze(1).repeat(1, height, 1)\n",
    "        pe[1:d_model_half:2, :, :] = torch.cos(pos_w * div_term).transpose(0, 1).unsqueeze(1).repeat(1, height, 1)\n",
    "        pe[d_model_half::2, :, :] = torch.sin(pos_h * div_term).transpose(0, 1).unsqueeze(2).repeat(1, 1, width)\n",
    "        pe[d_model_half+1::2, :, :] = torch.cos(pos_h * div_term).transpose(0, 1).unsqueeze(2).repeat(1, 1, width)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :x.size(2), :x.size(3)]\n",
    "\n",
    "# --- 5. Hybrid Attention-Transformer U-Net ---\n",
    "class HybridAttentionTransformerUNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, features=[64, 128, 256, 512],\n",
    "                 transformer_embed_dim=512, transformer_num_heads=8, transformer_ff_dim=2048,\n",
    "                 transformer_num_layers=4, transformer_dropout=0.1, bottleneck_img_size=(16,16)):\n",
    "        super(HybridAttentionTransformerUNet, self).__init__()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.bottleneck_img_h, self.bottleneck_img_w = bottleneck_img_size\n",
    "\n",
    "        if features[-1] != transformer_embed_dim:\n",
    "            raise ValueError(f\"Last feature size ({features[-1]}) must match transformer_embed_dim ({transformer_embed_dim})\")\n",
    "\n",
    "        current_in_channels = in_channels\n",
    "        for feature in features:\n",
    "            self.downs.append(ConvBlock(current_in_channels, feature))\n",
    "            current_in_channels = feature\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding2D(transformer_embed_dim, self.bottleneck_img_h, self.bottleneck_img_w)\n",
    "        self.transformer_encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoderBlock(transformer_embed_dim, transformer_num_heads, transformer_ff_dim, transformer_dropout)\n",
    "            for _ in range(transformer_num_layers)\n",
    "        ])\n",
    "\n",
    "        self.attentions = nn.ModuleList()\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(nn.ConvTranspose2d(feature * 2, feature, kernel_size=2, stride=2))\n",
    "            self.attentions.append(AttentionGate(F_g=feature, F_l=feature, F_int=feature // 2))\n",
    "            self.ups.append(ConvBlock(feature * 2, feature))\n",
    "\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "        for i, down_block in enumerate(self.downs):\n",
    "            x = down_block(x)\n",
    "            skip_connections.append(x)\n",
    "            if i < len(self.downs) - 1:\n",
    "                 x = self.pool(x)\n",
    "\n",
    "        if x.shape[2] != self.bottleneck_img_h or x.shape[3] != self.bottleneck_img_w:\n",
    "            x = F.adaptive_avg_pool2d(x, (self.bottleneck_img_h, self.bottleneck_img_w))\n",
    "\n",
    "        x = self.pos_encoder(x)\n",
    "        batch_size, channels, height, width = x.shape\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        for transformer_layer in self.transformer_encoder_layers:\n",
    "            x = transformer_layer(x)\n",
    "        x = x.transpose(1, 2).reshape(batch_size, channels, height, width)\n",
    "\n",
    "        skip_connections = skip_connections[::-1]\n",
    "        for i in range(0, len(self.ups), 2):\n",
    "            x = self.ups[i](x)\n",
    "            skip_connection = skip_connections[i//2]\n",
    "            if x.shape != skip_connection.shape:\n",
    "                 x = F.interpolate(x, size=skip_connection.shape[2:], mode='bilinear', align_corners=False)\n",
    "            attention_map = self.attentions[i//2](g=x, x=skip_connection)\n",
    "            concat_skip = torch.cat((attention_map, x), dim=1)\n",
    "            x = self.ups[i+1](concat_skip)\n",
    "        x = self.final_conv(x)\n",
    "        if self.final_conv.out_channels == 1:\n",
    "            return torch.sigmoid(x)\n",
    "        else:\n",
    "            return x\n",
    "# --- End of Model Definition ---\n",
    "\n",
    "\n",
    "# --- Configuration & Hyperparameters ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 4 # Adjust based on your GPU memory\n",
    "NUM_EPOCHS = 50 # Adjust as needed\n",
    "IMAGE_HEIGHT = 256 # Desired input height\n",
    "IMAGE_WIDTH = 256  # Desired input width\n",
    "NUM_WORKERS = 2 # For DataLoader\n",
    "\n",
    "# --- IMPORTANT: Paths to your data ---\n",
    "# Make sure these paths are correct and the CSV file is structured as described.\n",
    "CSV_FILE_PATH = \"square_dataset/dataset.csv\"  # Replace with your CSV file path\n",
    "IMAGE_DIR_ROOT = \"square_dataset\" # Base directory if paths in CSV are relative\n",
    "# Example: if CSV has 'subject1/nir_01.png' and IMAGE_DIR_ROOT is '/data/veins/',\n",
    "# then full path is '/data/veins/subject1/nir_01.png'\n",
    "# If paths in CSV are absolute, IMAGE_DIR_ROOT can be an empty string or not used.\n",
    "\n",
    "# Choose which image type to use from CSV: 'nir_image' or 'preprocessed_image'\n",
    "INPUT_IMAGE_COLUMN = 'preprocessed_images' # or 'preprocessed_image'\n",
    "MASK_COLUMN = 'mask'\n",
    "\n",
    "# --- Dataset Class ---\n",
    "class VeinDataset(Dataset):\n",
    "    def __init__(self, csv_file, image_dir_root, image_column, mask_column, transform=None, is_train=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            image_dir_root (string): Directory with all the images if paths in CSV are relative.\n",
    "            image_column (string): Column name in CSV for input image paths.\n",
    "            mask_column (string): Column name in CSV for mask image paths.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            is_train (bool): If true, dataset is for training (used for specific augmentations).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.data_frame = pd.read_csv(csv_file)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: CSV file not found at {csv_file}\")\n",
    "            print(\"Please ensure the CSV_FILE_PATH is correct.\")\n",
    "            raise\n",
    "        \n",
    "        self.image_dir_root = image_dir_root\n",
    "        self.image_column = image_column\n",
    "        self.mask_column = mask_column\n",
    "        self.transform = transform\n",
    "        self.is_train = is_train\n",
    "\n",
    "        # Correct typo in 'genere' if it exists in your actual CSV, otherwise use 'gender'\n",
    "        # For this example, we'll assume the CSV has 'gender' or you've corrected 'genere'\n",
    "        # self.data_frame.rename(columns={'genere': 'gender'}, inplace=True, errors='ignore')\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name_relative = self.data_frame.loc[idx, self.image_column]\n",
    "        mask_name_relative = self.data_frame.loc[idx, self.mask_column]\n",
    "\n",
    "        # Construct full paths\n",
    "        img_path = os.path.join(self.image_dir_root, img_name_relative) if self.image_dir_root else img_name_relative\n",
    "        mask_path = os.path.join(self.image_dir_root, mask_name_relative) if self.image_dir_root else mask_name_relative\n",
    "        \n",
    "        try:\n",
    "            # Load image (ensure it's loaded as grayscale if in_channels=1)\n",
    "            image = np.array(Image.open(img_path).convert(\"L\")) # \"L\" for grayscale\n",
    "            # Load mask (ensure it's loaded as grayscale)\n",
    "            mask = np.array(Image.open(mask_path).convert(\"L\")) # \"L\" for grayscale\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Error loading image or mask at index {idx}: {e}\")\n",
    "            print(f\"Attempted image path: {img_path}\")\n",
    "            print(f\"Attempted mask path: {mask_path}\")\n",
    "            # Return None or raise error, or return a placeholder\n",
    "            # For simplicity, we'll raise it here. Consider more robust error handling.\n",
    "            raise\n",
    "\n",
    "        # Binarize mask: Veins are often white (255) and background black (0).\n",
    "        # Adjust threshold if necessary based on your mask format.\n",
    "        mask[mask == 255.0] = 1.0 # Vein class\n",
    "        mask[mask != 1.0] = 0.0   # Background class\n",
    "        mask = mask.astype(np.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "        \n",
    "        # Ensure mask is (1, H, W) and not (H, W) for BCEWithLogitsLoss\n",
    "        if mask.ndim == 2:\n",
    "            mask = mask.unsqueeze(0)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# --- Transforms/Augmentations ---\n",
    "# Define different transforms for training and validation\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "    A.Rotate(limit=35, p=0.5),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.1),\n",
    "    A.Normalize(\n",
    "        mean=[0.0], # For grayscale, use single mean/std\n",
    "        std=[1.0],  # Or calculate from your dataset: image.mean()/255.0, image.std()/255.0\n",
    "        max_pixel_value=255.0,\n",
    "    ),\n",
    "    ToTensorV2(), # Converts image and mask to PyTorch tensors\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "    A.Normalize(\n",
    "        mean=[0.0],\n",
    "        std=[1.0],\n",
    "        max_pixel_value=255.0,\n",
    "    ),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "\n",
    "# --- Loss Function ---\n",
    "# BCEWithLogitsLoss is common for binary segmentation.\n",
    "# Consider Dice Loss or a combination for better handling of class imbalance.\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        inputs = torch.sigmoid(inputs) # Apply sigmoid if model outputs logits\n",
    "\n",
    "        # Flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice = (2. * intersection + self.smooth) / (inputs.sum() + targets.sum() + self.smooth)\n",
    "        return 1 - dice\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def load_checkpoint(checkpoint, model, optimizer=None):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    if optimizer and 'optimizer' in checkpoint:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    return checkpoint.get('epoch', 0) # Return epoch if saved\n",
    "\n",
    "def check_accuracy(loader, model, loss_fn, device=\"cuda\"):\n",
    "    num_correct = 0\n",
    "    num_pixels = 0\n",
    "    dice_score_val = 0\n",
    "    total_loss = 0\n",
    "    model.eval() # Set model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device=device) # y should be (N, 1, H, W)\n",
    "\n",
    "            preds_logits = model(x)\n",
    "            loss = loss_fn(preds_logits, y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds_probs = torch.sigmoid(preds_logits)\n",
    "            preds_binary = (preds_probs > 0.5).float()\n",
    "\n",
    "            num_correct += (preds_binary == y).sum()\n",
    "            num_pixels += torch.numel(preds_binary) # Total pixels\n",
    "\n",
    "            # Calculate Dice score for batch\n",
    "            intersection = (preds_binary * y).sum()\n",
    "            dice_batch = (2. * intersection) / (preds_binary.sum() + y.sum() + 1e-6) # Add smooth\n",
    "            dice_score_val += dice_batch.item()\n",
    "\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    pixel_accuracy = num_correct / num_pixels * 100\n",
    "    avg_dice_score = dice_score_val / len(loader)\n",
    "    \n",
    "    print(f\"Validation: Got {num_correct}/{num_pixels} with acc {pixel_accuracy:.2f}%\")\n",
    "    print(f\"Validation Dice score: {avg_dice_score:.4f}\")\n",
    "    print(f\"Validation Avg Loss: {avg_loss:.4f}\")\n",
    "    model.train() # Set model back to train mode\n",
    "    return avg_loss, pixel_accuracy, avg_dice_score\n",
    "\n",
    "# --- Training Function ---\n",
    "def train_fn(loader, model, optimizer, loss_fn, scaler=None): # Add scaler for mixed precision\n",
    "    loop = loader # tqdm(loader, leave=True) # Consider using tqdm for progress bar\n",
    "    mean_loss = []\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(loop):\n",
    "        data = data.to(device=DEVICE)\n",
    "        targets = targets.to(device=DEVICE).float() # Ensure targets are float\n",
    "\n",
    "        # Forward\n",
    "        if scaler: # Mixed precision\n",
    "            with torch.cuda.amp.autocast():\n",
    "                predictions = model(data)\n",
    "                loss = loss_fn(predictions, targets)\n",
    "        else: # Standard precision\n",
    "            predictions = model(data)\n",
    "            loss = loss_fn(predictions, targets)\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        if scaler:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        mean_loss.append(loss.item())\n",
    "        # loop.set_postfix(loss=loss.item()) # If using tqdm\n",
    "\n",
    "        if batch_idx % 50 == 0: # Print loss every 50 batches\n",
    "            print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Batch [{batch_idx}/{len(loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    print(f\"Mean loss for epoch: {sum(mean_loss)/len(mean_loss):.4f}\")\n",
    "\n",
    "\n",
    "# --- Main Function ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    # Calculate bottleneck image size for the model\n",
    "    # Assuming 4 pooling layers for features=[64, 128, 256, 512]\n",
    "    num_pooling_layers = 4 # Adjust if your 'features' list implies different depth\n",
    "    bottleneck_h = IMAGE_HEIGHT // (2**num_pooling_layers)\n",
    "    bottleneck_w = IMAGE_WIDTH // (2**num_pooling_layers)\n",
    "\n",
    "    model = HybridAttentionTransformerUNet(\n",
    "        in_channels=1,  # Grayscale input\n",
    "        out_channels=1, # Binary mask output\n",
    "        features=[64, 128, 256, 512],\n",
    "        transformer_embed_dim=512,\n",
    "        transformer_num_heads=8,\n",
    "        transformer_ff_dim=2048,\n",
    "        transformer_num_layers=4, # Adjust as needed\n",
    "        bottleneck_img_size=(bottleneck_h, bottleneck_w)\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # Choose loss function\n",
    "    # loss_fn = nn.BCEWithLogitsLoss() # Good starting point\n",
    "    loss_fn = DiceLoss() # Often better for segmentation\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # Optional: Gradient Scaler for mixed-precision training (if using CUDA)\n",
    "    scaler = None\n",
    "    if DEVICE == \"cuda\":\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # --- Prepare DataLoaders ---\n",
    "    # You might want to split your CSV into train/val sets\n",
    "    # For simplicity, using the whole CSV for both here.\n",
    "    # In practice, create separate CSVs or split dataframe.\n",
    "    # Example:\n",
    "    # df = pd.read_csv(CSV_FILE_PATH)\n",
    "    # train_df = df.sample(frac=0.8, random_state=42)\n",
    "    # val_df = df.drop(train_df.index)\n",
    "    # train_df.to_csv(\"train_data.csv\", index=False)\n",
    "    # val_df.to_csv(\"val_data.csv\", index=False)\n",
    "    # Then use \"train_data.csv\" and \"val_data.csv\" for VeinDataset\n",
    "\n",
    "    try:\n",
    "        train_dataset = VeinDataset(\n",
    "            csv_file=CSV_FILE_PATH, # Replace with your training CSV if split\n",
    "            image_dir_root=IMAGE_DIR_ROOT,\n",
    "            image_column=INPUT_IMAGE_COLUMN,\n",
    "            mask_column=MASK_COLUMN,\n",
    "            transform=train_transform,\n",
    "            is_train=True\n",
    "        )\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=True,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "        val_dataset = VeinDataset(\n",
    "            csv_file=CSV_FILE_PATH, # Replace with your validation CSV if split\n",
    "            image_dir_root=IMAGE_DIR_ROOT,\n",
    "            image_column=INPUT_IMAGE_COLUMN,\n",
    "            mask_column=MASK_COLUMN,\n",
    "            transform=val_transform,\n",
    "            is_train=False\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=BATCH_SIZE, # Can often be larger for validation\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=True,\n",
    "            shuffle=False,\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        print(\"Exiting due to CSV file not found. Please check paths in the script.\")\n",
    "        exit()\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating datasets/dataloaders: {e}\")\n",
    "        exit()\n",
    "\n",
    "\n",
    "    # --- Optional: Load Checkpoint ---\n",
    "    # start_epoch = 0\n",
    "    # if os.path.exists(\"my_checkpoint.pth.tar\"):\n",
    "    #     start_epoch = load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n",
    "    #     print(f\"Resuming training from epoch {start_epoch+1}\")\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    best_val_dice = 0.0 # To save the best model based on validation Dice\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS): # Use start_epoch if resuming\n",
    "        print(f\"\\n--- Epoch {epoch+1}/{NUM_EPOCHS} ---\")\n",
    "        train_fn(train_loader, model, optimizer, loss_fn, scaler)\n",
    "\n",
    "        # Perform validation\n",
    "        val_loss, val_pixel_acc, val_dice = check_accuracy(val_loader, model, loss_fn, device=DEVICE)\n",
    "\n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }\n",
    "        save_checkpoint(checkpoint, filename=f\"checkpoint_epoch_{epoch+1}.pth.tar\")\n",
    "\n",
    "        # Save the best model based on validation dice score\n",
    "        if val_dice > best_val_dice:\n",
    "            print(f\"New best validation Dice score: {val_dice:.4f} (previous: {best_val_dice:.4f})\")\n",
    "            best_val_dice = val_dice\n",
    "            save_checkpoint(checkpoint, filename=\"best_model_checkpoint.pth.tar\")\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "    print(f\"Best validation Dice score achieved: {best_val_dice:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "college",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
